{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":32267,"sourceType":"datasetVersion","datasetId":24984}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm, tqdm_notebook\nfrom glob import glob\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-24T18:26:10.149693Z","iopub.execute_input":"2023-12-24T18:26:10.149952Z","iopub.status.idle":"2023-12-24T18:26:10.503768Z","shell.execute_reply.started":"2023-12-24T18:26:10.149928Z","shell.execute_reply":"2023-12-24T18:26:10.502822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"# Show the directory structure\nroot_dir = \"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles\"\n\n# Show the folders inside the root\nprint(os.listdir(root_dir))","metadata":{"execution":{"iopub.status.busy":"2023-12-24T03:50:50.079168Z","iopub.execute_input":"2023-12-24T03:50:50.079511Z","iopub.status.idle":"2023-12-24T03:50:50.102725Z","shell.execute_reply.started":"2023-12-24T03:50:50.079484Z","shell.execute_reply":"2023-12-24T03:50:50.101733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List which has the unique category names\nunique_category = os.listdir(root_dir)\n\n# List to store the category names\nnews_category_list = []\n\n# List to store the news articles\nnews_article_list = []\n\nnews_summaries_list = []\n\n# Iterate through unique category\nfor category in tqdm(unique_category, colour='yellow'):\n    # Get the absolute path of that category directory\n    abs_category_path = root_dir + \"/\" + category\n    abs_summary_path = \"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries\" + \"/\" + category\n    \n    # Create a query to get the absolute path of all the .txt files\n    query = abs_category_path + \"/*.txt\"\n    summary_query = abs_summary_path + \"/*.txt\"\n    #print(query)\n    \n    # Get the list of absolute path of all the files\n    file_paths = glob(query)\n    summary_paths = glob(summary_query)\n    \n    # Iterate through all the .txt files path\n    for file_path in file_paths:\n\n        # Open the file\n        f = open(file_path, 'r', encoding='latin-1')\n        # Read the file contents\n        news_article = f.read()\n        # Close the file\n        f.close()\n\n            \n        news_article_list.append(news_article)\n        news_category_list.append(category)\n            \n    for summary_path in summary_paths:\n        # Open the file\n        f = open(summary_path, 'r', encoding='utf-8')\n        # Read the file contents\n        news_summary = f.read()\n        # Close the file\n        f.close()\n\n            \n        # Append new articles and category to respective list\n        news_summaries_list.append(news_summary)\n        \n# Print length of news articles and categories\nprint(\"Total Articles: \", len(news_article_list))\nprint(\"Total Summaries: \", len(news_summaries_list))\nprint(\"Total Categories: \", len(news_category_list), end='\\n\\n')\n\n# Print a sample article\nprint(\"Sample Article: \")\nprint(news_article_list[0])","metadata":{"execution":{"iopub.status.busy":"2023-12-24T03:50:51.548936Z","iopub.execute_input":"2023-12-24T03:50:51.549896Z","iopub.status.idle":"2023-12-24T03:51:20.413193Z","shell.execute_reply.started":"2023-12-24T03:50:51.549846Z","shell.execute_reply":"2023-12-24T03:51:20.412326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dictionary to create a dataframe\ndf_dict = {\"news\": news_article_list, \"summaries\":news_summaries_list, \"labels\": news_category_list}\n\n# Convert to dataframe\ndf = pd.DataFrame(df_dict)\n\n# Remove the 'n' from the news\ndef remove_ns(text):\n    return text.replace(\"\\n\", \" \")\n\n# Apply this to the data frame\ndf['news'] = df['news'].astype(str)\ndf['summaries'] = df['summaries'].astype(str)\nfor i in range(df.shape[0]):\n    df.news[i] = df.news[i].replace(\"\\n\", \" \")\n\n# Show the dataframe\ndf.head()\n\n\n# Save the df into system\ndf.to_csv('/kaggle/working/origin_dataframe.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T03:51:22.831860Z","iopub.execute_input":"2023-12-24T03:51:22.832556Z","iopub.status.idle":"2023-12-24T03:51:23.735103Z","shell.execute_reply.started":"2023-12-24T03:51:22.832522Z","shell.execute_reply":"2023-12-24T03:51:23.734034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working/origin_dataframe.csv')\ndf.head()\n#df.summaries[3]\n#df.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-24T03:51:27.055402Z","iopub.execute_input":"2023-12-24T03:51:27.055755Z","iopub.status.idle":"2023-12-24T03:51:27.144216Z","shell.execute_reply.started":"2023-12-24T03:51:27.055725Z","shell.execute_reply":"2023-12-24T03:51:27.143323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task B","metadata":{}},{"cell_type":"markdown","source":"## Prepare Dataset for training the Model A&B","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n\ndf = pd.read_csv('/kaggle/working/origin_dataframe.csv')\ndf_business=df[df['labels'] == 'business']\nten_percent = int(0.1 * len(df_business))\ntest_data = df_business.sample(n=ten_percent, random_state=42)\n\ndf = df.drop(test_data.index)\ndf_business = df_business.drop(test_data.index)\nmodelB_dataset=Dataset.from_pandas(df_business)\n#modelA_dataset=Dataset.from_pandas(df_business)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T03:52:53.491035Z","iopub.execute_input":"2023-12-24T03:52:53.491826Z","iopub.status.idle":"2023-12-24T03:52:54.599502Z","shell.execute_reply.started":"2023-12-24T03:52:53.491798Z","shell.execute_reply":"2023-12-24T03:52:54.598722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to ensure that the amount of business data used to train model A is no more than B, 20% of the business data is actively deleted.","metadata":{}},{"cell_type":"code","source":"business_data = df[df['labels'] == 'business']\n\ntwenty_percent = int(0.2 * len(business_data))\n\nindices_to_remove = business_data.sample(n=twenty_percent, random_state=42).index\n\ndf = df.drop(indices_to_remove)\nprint(df[df['labels'] == 'business'].shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T03:53:06.655509Z","iopub.execute_input":"2023-12-24T03:53:06.655880Z","iopub.status.idle":"2023-12-24T03:53:06.666701Z","shell.execute_reply.started":"2023-12-24T03:53:06.655849Z","shell.execute_reply":"2023-12-24T03:53:06.665582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelA_dataset=Dataset.from_pandas(df)\ntest_dataset=Dataset.from_pandas(test_data)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T03:53:14.894648Z","iopub.execute_input":"2023-12-24T03:53:14.895029Z","iopub.status.idle":"2023-12-24T03:53:14.918537Z","shell.execute_reply.started":"2023-12-24T03:53:14.894998Z","shell.execute_reply":"2023-12-24T03:53:14.917566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nimport torch\n\n#configuration = BartConfig()\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n\ninputs = tokenizer(\"I loved reading the Hunger Games!\")","metadata":{"execution":{"iopub.status.busy":"2023-12-24T03:54:31.260562Z","iopub.execute_input":"2023-12-24T03:54:31.260930Z","iopub.status.idle":"2023-12-24T03:54:42.750559Z","shell.execute_reply.started":"2023-12-24T03:54:31.260902Z","shell.execute_reply":"2023-12-24T03:54:42.749555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_input_length = 700\nmax_target_length = 500\n\ndef preprocess_function(examples):\n\n    model_inputs = tokenizer(\n        examples[\"news\"], max_length=max_input_length, padding=\"max_length\",\n        truncation=True\n    )\n    # Set up the tokenizer for targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"summaries\"], max_length=max_target_length, padding=\"max_length\",\n            truncation=True\n        )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2023-12-24T03:54:45.088528Z","iopub.execute_input":"2023-12-24T03:54:45.089080Z","iopub.status.idle":"2023-12-24T03:54:45.095024Z","shell.execute_reply.started":"2023-12-24T03:54:45.089048Z","shell.execute_reply":"2023-12-24T03:54:45.094038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_modelA_datasets = modelA_dataset.map(preprocess_function, batched=False)\ntokenized_modelB_datasets = modelB_dataset.map(preprocess_function, batched=False)\ntokenized_test_datasets = test_dataset.map(preprocess_function, batched=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T03:54:47.774022Z","iopub.execute_input":"2023-12-24T03:54:47.774851Z","iopub.status.idle":"2023-12-24T03:55:01.613483Z","shell.execute_reply.started":"2023-12-24T03:54:47.774816Z","shell.execute_reply":"2023-12-24T03:55:01.612682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets.dataset_dict import DatasetDict\n\ntokenized_modelA_datasets_dict=tokenized_modelA_datasets.train_test_split(test_size=0.1, shuffle = True)\ntokenized_modelB_datasets_dict=tokenized_modelB_datasets.train_test_split(test_size=0.05, shuffle = True)\n\ntokenized_modelA_datasets_dict = DatasetDict({\n    'train': tokenized_modelA_datasets_dict['train'],\n    'validation': tokenized_modelA_datasets_dict['test'],\n    'test': tokenized_test_datasets})\ntokenized_modelB_datasets_dict = DatasetDict({\n    'train': tokenized_modelB_datasets_dict['train'],\n    'validation': tokenized_modelB_datasets_dict['test'],\n    'test': tokenized_test_datasets})\n\nprint(tokenized_modelA_datasets_dict)\nprint(tokenized_modelB_datasets_dict)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T03:56:06.252450Z","iopub.execute_input":"2023-12-24T03:56:06.252813Z","iopub.status.idle":"2023-12-24T03:56:07.999116Z","shell.execute_reply.started":"2023-12-24T03:56:06.252787Z","shell.execute_reply":"2023-12-24T03:56:07.998216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save the dataset for future training","metadata":{}},{"cell_type":"code","source":"tokenized_modelA_datasets_dict.save_to_disk(\"/kaggle/working/dataA\") \ntokenized_modelB_datasets_dict.save_to_disk(\"/kaggle/working/dataB\") ","metadata":{"execution":{"iopub.status.busy":"2023-12-24T03:56:17.714916Z","iopub.execute_input":"2023-12-24T03:56:17.715873Z","iopub.status.idle":"2023-12-24T03:56:21.706538Z","shell.execute_reply.started":"2023-12-24T03:56:17.715842Z","shell.execute_reply":"2023-12-24T03:56:21.705736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read datasetDict","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm, tqdm_notebook\nfrom glob import glob\nimport os\nfrom datasets import Dataset\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport torch\n\n#configuration = BartConfig()\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n\nfrom datasets import load_from_disk\ntokenized_modelA_datasets_dict = load_from_disk(\"/kaggle/working/dataA\")\ntokenized_modelB_datasets_dict = load_from_disk(\"/kaggle/working/dataB\")","metadata":{"execution":{"iopub.status.busy":"2023-12-26T04:33:25.625658Z","iopub.execute_input":"2023-12-26T04:33:25.626329Z","iopub.status.idle":"2023-12-26T04:33:39.035067Z","shell.execute_reply.started":"2023-12-26T04:33:25.626294Z","shell.execute_reply":"2023-12-26T04:33:39.034286Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74e1133a9058453bac9fc53101595155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"993863535c804865b24756961f0824f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4be3ca48b72845b182336a6ac260ce1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee443975d0764727a8a3445636f9c38e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f670699ab22f455791db22e4a269f714"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d19f3bd0dfd464a8d8117f1100c0c4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9139534d2354bdd8b9c86488d5e8183"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized_modelA_datasets_dict ","metadata":{"execution":{"iopub.status.busy":"2023-12-26T04:37:46.704604Z","iopub.execute_input":"2023-12-26T04:37:46.705246Z","iopub.status.idle":"2023-12-26T04:37:46.712574Z","shell.execute_reply.started":"2023-12-26T04:37:46.705213Z","shell.execute_reply":"2023-12-26T04:37:46.711674Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['news', 'summaries', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 1874\n    })\n    validation: Dataset({\n        features: ['news', 'summaries', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 209\n    })\n    test: Dataset({\n        features: ['news', 'summaries', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 51\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_modelB_datasets_dict","metadata":{"execution":{"iopub.status.busy":"2023-12-26T04:37:49.549497Z","iopub.execute_input":"2023-12-26T04:37:49.549881Z","iopub.status.idle":"2023-12-26T04:37:49.556342Z","shell.execute_reply.started":"2023-12-26T04:37:49.549849Z","shell.execute_reply":"2023-12-26T04:37:49.555277Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['news', 'summaries', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 436\n    })\n    validation: Dataset({\n        features: ['news', 'summaries', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 23\n    })\n    test: Dataset({\n        features: ['news', 'summaries', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 51\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Train a specific model B (business data)\n### Data Analysis","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.tokenize import word_tokenize \n\nax = df_business['labels'].value_counts().plot(kind='bar', figsize=(10,7))\n\n#for p in ax.patches:\n#    ax.annotate(\"{:.1f}\".format(p.get_height()), (p.get_x(), p.get_height()+5))\nplt.title(\"News categories count\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T02:10:50.264228Z","iopub.execute_input":"2023-12-24T02:10:50.264638Z","iopub.status.idle":"2023-12-24T02:10:50.521304Z","shell.execute_reply.started":"2023-12-24T02:10:50.264599Z","shell.execute_reply":"2023-12-24T02:10:50.519954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_counts = [len(word_tokenize(text)) for text in df_business.news]\n\nplt.figure(figsize=(10, 6))\nsns.histplot(token_counts, bins=50, kde=True, color='blue')\nplt.title('Distribution of Token Counts in Texts')\nplt.xlabel('Number of Tokens')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T02:10:54.844873Z","iopub.execute_input":"2023-12-24T02:10:54.845255Z","iopub.status.idle":"2023-12-24T02:10:56.392532Z","shell.execute_reply.started":"2023-12-24T02:10:54.845226Z","shell.execute_reply":"2023-12-24T02:10:56.391361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_counts_summary = [len(word_tokenize(text_sum)) for text_sum in df_business.summaries]\n\nplt.figure(figsize=(10, 6))\nsns.histplot(token_counts_summary, bins=50, kde=True, color='blue')\nplt.title('Distribution of Token Counts in Summaries')\nplt.xlabel('Number of Tokens')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T19:37:58.233738Z","iopub.execute_input":"2023-12-22T19:37:58.234161Z","iopub.status.idle":"2023-12-22T19:38:00.766098Z","shell.execute_reply.started":"2023-12-22T19:37:58.234127Z","shell.execute_reply":"2023-12-22T19:38:00.764918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection\n\nGiven that the large model was likely pre-trained on our dataset, we chose to use a smaller language model.","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nimport torch\n\n#configuration = BartConfig()\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n\ninputs = tokenizer(\"I loved reading the Hunger Games!\")","metadata":{"execution":{"iopub.status.busy":"2023-12-26T04:34:00.666704Z","iopub.execute_input":"2023-12-26T04:34:00.667322Z","iopub.status.idle":"2023-12-26T04:34:02.188792Z","shell.execute_reply.started":"2023-12-26T04:34:00.667284Z","shell.execute_reply":"2023-12-26T04:34:02.187862Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine-tune the model B","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nfrom datasets import load_dataset\nimport time\nimport torch.optim as optim\n#import evaluate","metadata":{"execution":{"iopub.status.busy":"2023-12-26T04:34:04.892667Z","iopub.execute_input":"2023-12-26T04:34:04.893124Z","iopub.status.idle":"2023-12-26T04:34:16.032893Z","shell.execute_reply.started":"2023-12-26T04:34:04.893089Z","shell.execute_reply":"2023-12-26T04:34:16.031918Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(model))","metadata":{"execution":{"iopub.status.busy":"2023-12-26T04:34:19.890850Z","iopub.execute_input":"2023-12-26T04:34:19.891740Z","iopub.status.idle":"2023-12-26T04:34:19.901977Z","shell.execute_reply.started":"2023-12-26T04:34:19.891705Z","shell.execute_reply":"2023-12-26T04:34:19.899927Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"trainable model parameters: 247577856\nall model parameters: 247577856\npercentage of trainable model parameters: 100.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Setup the PEFT/LoRA model for Fine-Tuning","metadata":{}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model_B))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config_B = LoraConfig(\n    r=8, # Rank\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n)\n\n#Add LoRA adapter layers/parameters to the original LLM to be trained.\npeft_model_B = get_peft_model(model, \n                            lora_config_B)\n\nprint(print_number_of_trainable_model_parameters(peft_model_B))","metadata":{"execution":{"iopub.status.busy":"2023-12-26T04:35:12.557670Z","iopub.execute_input":"2023-12-26T04:35:12.558081Z","iopub.status.idle":"2023-12-26T04:35:12.753101Z","shell.execute_reply.started":"2023-12-26T04:35:12.558046Z","shell.execute_reply":"2023-12-26T04:35:12.752184Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"trainable model parameters: 884736\nall model parameters: 248462592\npercentage of trainable model parameters: 0.36%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train PEFT Adapter B","metadata":{}},{"cell_type":"code","source":"output_dir = f'/kaggle/working/peft-dialogue-summary-training-{str(int(time.time()))}'\n\npeft_training_args_B = TrainingArguments(\n    output_dir=output_dir,\n    auto_find_batch_size=True,\n    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n    num_train_epochs=10,\n    save_steps=10000,\n    logging_steps=100\n)\n\npeft_trainer_B = Trainer(\n    model=peft_model_B,\n    args=peft_training_args_B,\n    train_dataset=tokenized_modelB_datasets_dict[\"train\"],\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T02:38:17.320283Z","iopub.execute_input":"2023-12-25T02:38:17.320662Z","iopub.status.idle":"2023-12-25T02:38:17.593556Z","shell.execute_reply.started":"2023-12-25T02:38:17.320631Z","shell.execute_reply":"2023-12-25T02:38:17.592491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from safetensors.torch import load_model, save_model\npeft_trainer_B.train()\n\npeft_model_path_B=\"/kaggle/working/peft-dialogue-summary-checkpoint-local-B\"\n\npeft_trainer_B.model.save_pretrained(peft_model_path_B)\ntokenizer.save_pretrained(peft_model_path_B)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T02:57:19.516425Z","iopub.execute_input":"2023-12-25T02:57:19.517245Z","iopub.status.idle":"2023-12-25T03:10:56.255326Z","shell.execute_reply.started":"2023-12-25T02:57:19.517210Z","shell.execute_reply":"2023-12-25T03:10:56.254453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n\npeft_model_B = PeftModel.from_pretrained(model, \n                                       '/kaggle/working/peft-dialogue-summary-checkpoint-local-B', \n                                       torch_dtype=torch.bfloat16,\n                                       is_trainable=True)\n\npeft_model_base = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:11:12.725638Z","iopub.execute_input":"2023-12-25T03:11:12.726019Z","iopub.status.idle":"2023-12-25T03:11:15.157389Z","shell.execute_reply.started":"2023-12-25T03:11:12.725983Z","shell.execute_reply":"2023-12-25T03:11:15.156293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model_B))\nprint(print_number_of_trainable_model_parameters(peft_model_base))","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:11:16.101955Z","iopub.execute_input":"2023-12-25T03:11:16.102677Z","iopub.status.idle":"2023-12-25T03:11:16.121220Z","shell.execute_reply.started":"2023-12-25T03:11:16.102645Z","shell.execute_reply":"2023-12-25T03:11:16.120300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the Model Quantitatively (with ROUGE Metric)","metadata":{}},{"cell_type":"code","source":"#human_baseline_summaries = train_test_val_data['validation']['summaries']\n\nhuman_baseline_summaries = tokenized_modelB_datasets_dict[\"validation\"]['summaries']\n\npeft_model_B_summaries = []\n\nfor i in range(len(human_baseline_summaries)):\n    news = torch.tensor([tokenized_modelB_datasets_dict[\"validation\"][\"input_ids\"][i],tokenized_modelB_datasets_dict[\"validation\"][\"attention_mask\"][i]])\n\n    peft_model_B_outputs = peft_model_B.generate(input_ids=news,\n                                             generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n    peft_model_B_text_output = tokenizer.decode(peft_model_B_outputs[0],\n                                              skip_special_tokens=True)\n\n    peft_model_B_summaries.append(peft_model_B_text_output)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:11:18.261525Z","iopub.execute_input":"2023-12-25T03:11:18.261889Z","iopub.status.idle":"2023-12-25T03:13:44.893000Z","shell.execute_reply.started":"2023-12-25T03:11:18.261863Z","shell.execute_reply":"2023-12-25T03:13:44.891986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load('rouge')\n\npeft_model_B_results = rouge.compute(\n    predictions=peft_model_B_summaries,\n    references=human_baseline_summaries[0:len(peft_model_B_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('PEFT MODEL:')\nprint(peft_model_B_results)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:15:54.955165Z","iopub.execute_input":"2023-12-25T03:15:54.955546Z","iopub.status.idle":"2023-12-25T03:15:56.126985Z","shell.execute_reply.started":"2023-12-25T03:15:54.955520Z","shell.execute_reply":"2023-12-25T03:15:56.123519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERTScore","metadata":{}},{"cell_type":"code","source":"from bert_score import BERTScorer\n\n# Instantiate the BERTScorer object for English language\nscorer = BERTScorer(lang=\"en\")\n\n# Calculate BERTScore for summary 2 against the excerpt\n# P2, R2, F2_2 represent Precision, Recall, and F1 Score respectively\nP2, R2, F2_2 = scorer.score(peft_model_B_summaries, human_baseline_summaries[0:len(peft_model_B_summaries)])\n\nprint(\"PEFT Model Summaries F1 Score:\", F2_2.tolist()[0])","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:16:29.241578Z","iopub.execute_input":"2023-12-25T03:16:29.241923Z","iopub.status.idle":"2023-12-25T03:16:31.135774Z","shell.execute_reply.started":"2023-12-25T03:16:29.241898Z","shell.execute_reply":"2023-12-25T03:16:31.134663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tune the model A","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config_A = LoraConfig(\n    r=32, # Rank\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n)\n\n#Add LoRA adapter layers/parameters to the original LLM to be trained.\npeft_model_A = get_peft_model(model, \n                            lora_config_A)\nprint(print_number_of_trainable_model_parameters(peft_model_A))","metadata":{"execution":{"iopub.status.busy":"2023-12-26T04:36:34.648766Z","iopub.execute_input":"2023-12-26T04:36:34.649539Z","iopub.status.idle":"2023-12-26T04:36:34.783026Z","shell.execute_reply.started":"2023-12-26T04:36:34.649503Z","shell.execute_reply":"2023-12-26T04:36:34.782117Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"trainable model parameters: 3538944\nall model parameters: 251116800\npercentage of trainable model parameters: 1.41%\n","output_type":"stream"}]},{"cell_type":"code","source":"output_dir = f'/kaggle/working/peft-dialogue-summary-training-{str(int(time.time()))}'\n\npeft_training_args_A = TrainingArguments(\n    output_dir=output_dir,\n    auto_find_batch_size=True,\n    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n    num_train_epochs=10,\n    save_steps=10000,\n    logging_steps=100\n)\n\npeft_trainer_A = Trainer(\n    model=peft_model_A,\n    args=peft_training_args_A,\n    train_dataset=tokenized_modelA_datasets_dict[\"train\"],\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T22:11:31.244306Z","iopub.execute_input":"2023-12-24T22:11:31.245219Z","iopub.status.idle":"2023-12-24T22:11:31.521573Z","shell.execute_reply.started":"2023-12-24T22:11:31.245181Z","shell.execute_reply":"2023-12-24T22:11:31.520678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from safetensors.torch import load_model, save_model\npeft_trainer_A.train()\n\npeft_model_path_A=\"/kaggle/working/peft-dialogue-summary-checkpoint-local-A\"\n\npeft_trainer_A.model.save_pretrained(peft_model_path_A)\ntokenizer.save_pretrained(peft_model_path_A)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T00:24:19.214615Z","iopub.execute_input":"2023-12-25T00:24:19.215252Z","iopub.status.idle":"2023-12-25T01:18:35.341967Z","shell.execute_reply.started":"2023-12-25T00:24:19.215219Z","shell.execute_reply":"2023-12-25T01:18:35.341027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n\npeft_model_A = PeftModel.from_pretrained(model, \n                                       '/kaggle/working/peft-dialogue-summary-checkpoint-local-A', \n                                       torch_dtype=torch.bfloat16,\n                                       is_trainable=True)\n\npeft_model_base = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T01:19:12.412364Z","iopub.execute_input":"2023-12-25T01:19:12.412881Z","iopub.status.idle":"2023-12-25T01:19:14.939802Z","shell.execute_reply.started":"2023-12-25T01:19:12.412845Z","shell.execute_reply":"2023-12-25T01:19:14.938480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model_A))\nprint(print_number_of_trainable_model_parameters(peft_model_base))","metadata":{"execution":{"iopub.status.busy":"2023-12-25T01:19:17.333343Z","iopub.execute_input":"2023-12-25T01:19:17.333822Z","iopub.status.idle":"2023-12-25T01:19:17.352717Z","shell.execute_reply.started":"2023-12-25T01:19:17.333789Z","shell.execute_reply":"2023-12-25T01:19:17.351736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ROUGE","metadata":{}},{"cell_type":"code","source":"#human_baseline_summaries = train_test_val_data['validation']['summaries']\n\nhuman_baseline_summaries = tokenized_modelA_datasets_dict[\"validation\"]['summaries']\n\npeft_model_A_summaries = []\n\nfor i in range(len(human_baseline_summaries)):\n    news = torch.tensor([tokenized_modelA_datasets_dict[\"validation\"][\"input_ids\"][i],tokenized_modelA_datasets_dict[\"validation\"][\"attention_mask\"][i]])\n\n    peft_model_A_outputs = peft_model_A.generate(input_ids=news,\n                                             generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n    peft_model_A_text_output = tokenizer.decode(peft_model_A_outputs[0],\n                                              skip_special_tokens=True)\n\n    peft_model_A_summaries.append(peft_model_A_text_output)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T01:19:20.884286Z","iopub.execute_input":"2023-12-25T01:19:20.884667Z","iopub.status.idle":"2023-12-25T01:21:58.861179Z","shell.execute_reply.started":"2023-12-25T01:19:20.884636Z","shell.execute_reply":"2023-12-25T01:21:58.860032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load('rouge')\n\npeft_model_A_results = rouge.compute(\n    predictions=peft_model_A_summaries,\n    references=human_baseline_summaries[0:len(peft_model_A_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('PEFT MODEL:')\nprint(peft_model_A_results)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T01:29:28.473196Z","iopub.execute_input":"2023-12-25T01:29:28.473954Z","iopub.status.idle":"2023-12-25T01:29:29.724194Z","shell.execute_reply.started":"2023-12-25T01:29:28.473913Z","shell.execute_reply":"2023-12-25T01:29:29.723140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERTScore","metadata":{}},{"cell_type":"code","source":"from bert_score import BERTScorer\n\n# Instantiate the BERTScorer object for English language\nscorer = BERTScorer(lang=\"en\")\n\n# Calculate BERTScore for summary 2 against the excerpt\n# P2, R2, F2_2 represent Precision, Recall, and F1 Score respectively\nP2, R2, F2_2 = scorer.score(peft_model_A_summaries, human_baseline_summaries[0:len(peft_model_A_summaries)])\n\nprint(\"PEFT Model Summaries F1 Score:\", F2_2.tolist()[0])","metadata":{"execution":{"iopub.status.busy":"2023-12-25T01:30:25.873066Z","iopub.execute_input":"2023-12-25T01:30:25.873431Z","iopub.status.idle":"2023-12-25T01:30:28.060077Z","shell.execute_reply.started":"2023-12-25T01:30:25.873402Z","shell.execute_reply":"2023-12-25T01:30:28.058969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate both model on the test set","metadata":{}},{"cell_type":"code","source":"human_baseline_summaries = tokenized_modelA_datasets_dict[\"test\"]['summaries']\n\npeft_model_A_summaries = []\npeft_model_B_summaries = []\n\nfor i in range(len(human_baseline_summaries)):\n    news = torch.tensor([tokenized_modelA_datasets_dict[\"test\"][\"input_ids\"][i],tokenized_modelA_datasets_dict[\"test\"][\"attention_mask\"][i]])\n\n    peft_model_A_outputs = peft_model_A.generate(input_ids=news,\n                                             generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n    peft_model_A_text_output = tokenizer.decode(peft_model_A_outputs[0],\n                                              skip_special_tokens=True)\n    \n    peft_model_B_outputs = peft_model_B.generate(input_ids=news,\n                                             generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n    peft_model_B_text_output = tokenizer.decode(peft_model_B_outputs[0],\n                                              skip_special_tokens=True)\n\n    peft_model_A_summaries.append(peft_model_A_text_output)\n    peft_model_B_summaries.append(peft_model_B_text_output)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:17:11.794932Z","iopub.execute_input":"2023-12-25T03:17:11.795954Z","iopub.status.idle":"2023-12-25T03:43:01.292804Z","shell.execute_reply.started":"2023-12-25T03:17:11.795904Z","shell.execute_reply":"2023-12-25T03:43:01.291743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ROUGE","metadata":{}},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load('rouge')\n\npeft_model_A_results = rouge.compute(\n    predictions=peft_model_A_summaries,\n    references=human_baseline_summaries[0:len(peft_model_A_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\npeft_model_B_results = rouge.compute(\n    predictions=peft_model_B_summaries,\n    references=human_baseline_summaries[0:len(peft_model_B_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('MODEL A:')\nprint(peft_model_A_results)\nprint('MODEL B:')\nprint(peft_model_B_results)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:45:01.548245Z","iopub.execute_input":"2023-12-25T03:45:01.548898Z","iopub.status.idle":"2023-12-25T03:45:06.132506Z","shell.execute_reply.started":"2023-12-25T03:45:01.548860Z","shell.execute_reply":"2023-12-25T03:45:06.131294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERTScore","metadata":{}},{"cell_type":"code","source":"from bert_score import BERTScorer\n\n# Instantiate the BERTScorer object for English language\nscorer = BERTScorer(lang=\"en\")\n\nP1, R1, F2_1 = scorer.score(peft_model_A_summaries, human_baseline_summaries[0:len(peft_model_A_summaries)])\nP2, R2, F2_2 = scorer.score(peft_model_B_summaries, human_baseline_summaries[0:len(peft_model_B_summaries)])\n\nprint(\"Model A Summaries F1 Score:\", F2_1.tolist()[0])\nprint(\"Model B Summaries F1 Score:\", F2_2.tolist()[0])","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:46:27.879366Z","iopub.execute_input":"2023-12-25T03:46:27.880043Z","iopub.status.idle":"2023-12-25T03:46:34.504264Z","shell.execute_reply.started":"2023-12-25T03:46:27.880010Z","shell.execute_reply":"2023-12-25T03:46:34.503093Z"},"trusted":true},"execution_count":null,"outputs":[]}]}